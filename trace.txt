╭─────────────────────────────── Traceback (most recent call last) ───────────────────────────────╮                                      
│ /home/--------------------------/prismer/eval_caption_v0.py:135 in <module>                      │
│                                                                                                  │
│   132 │   │                                                                                      │
│   133 │   │   if accelerator.is_main_process:                                                    │
│   134 │   │   │   for data_id, caption in zip(data_ids, captions):                               │
│ ❱ 135 │   │   │   │   caption = tokenizer.decode(caption, skip_special_tokens=True)              │
│   136 │   │   │   │   if args.target_dataset == 'coco':                                          │
│   137 │   │   │   │   │   image_id = int(test_loader.dataset.data_list[data_id]['image'].split   │
│   138 │   │   │   │   │   result.append({"image_id": image_id, "caption": caption.capitalize()   │
│                                                                                                  │
│ /home/-----------------------/lib/python3.8/site-packages/transformers/tokenization_utils_base.p │
│ y:3476 in decode                                                                                 │
│                                                                                                  │
│   3473 │   │   # Convert inputs to python lists                                                  │
│   3474 │   │   token_ids = to_py_obj(token_ids)                                                  │
│   3475 │   │                                                                                     │
│ ❱ 3476 │   │   return self._decode(                                                              │
│   3477 │   │   │   token_ids=token_ids,                                                          │
│   3478 │   │   │   skip_special_tokens=skip_special_tokens,                                      │
│   3479 │   │   │   clean_up_tokenization_spaces=clean_up_tokenization_spaces,                    │
│                                                                                                  │
│ /home/-----------------------/lib/python3.8/site-packages/transformers/tokenization_utils.py:931 │
│ in _decode                                                                                       │
│                                                                                                  │
│   928 │   ) -> str:                                                                              │
│   929 │   │   self._decode_use_source_tokenizer = kwargs.pop("use_source_tokenizer", False)      │
│   930 │   │                                                                                      │
│ ❱ 931 │   │   filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip   │
│   932 │   │                                                                                      │
│   933 │   │   # To avoid mixing byte-level and unicode for byte-level BPT                        │
│   934 │   │   # we need to build string separately for added tokens and byte-level tokens        │
│                                                                                                  │
│ /home/-----------------------/lib/python3.8/site-packages/transformers/tokenization_utils.py:906 │
│ in convert_ids_to_tokens                                                                         │
│                                                                                                  │
│   903 │   │   │   │   return self._convert_id_to_token(ids)                                      │
│   904 │   │   tokens = []                                                                        │
│   905 │   │   for index in ids:                                                                  │
│ ❱ 906 │   │   │   index = int(index)                                                             │
│   907 │   │   │   if skip_special_tokens and index in self.all_special_ids:                      │
│   908 │   │   │   │   continue                                                                   │
│   909 │   │   │   if index in self.added_tokens_decoder:                                         │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
ValueError: invalid literal for int() with base 10: 'a'